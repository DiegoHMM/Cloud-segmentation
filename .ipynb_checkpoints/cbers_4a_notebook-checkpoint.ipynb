{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras-unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import sklearn\n",
    "import random\n",
    "from osgeo import gdal, osr\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"D:/cbers_data/DataSetModelo/\"\n",
    "RAW_DATA_DIR = \"D:/cbers_data/DataSetModelo/raw_scene/\"\n",
    "RAW_MASK_DIR = \"D:/cbers_data/DataSetModelo/raw_mask/\"\n",
    "CLIPPED_DATA_DIR = \"D:/cbers_data/DataSetModelo/clip/\"\n",
    "VRT_DATA_DIR = \"D:/cbers_data/DataSetModelo/vrt/\"\n",
    "PREDICT_DIR = \"D:/cbers_data/DataSetModelo/predict/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibrating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"D:/cbers_data/DataSetModelo/CALIBRATING_MODEL/\"\n",
    "TRAINING_PATH = \"D:/cbers_data/DataSetModelo/CALIBRATING_MODEL/Training/\"\n",
    "SCENES_DIR = \"D:/cbers_data/DataSetModelo/CALIBRATING_MODEL/Scenes/\"\n",
    "MASKS_DIR = \"D:/cbers_data/DataSetModelo/CALIBRATING_MODEL/Masks/\"\n",
    "TRAINING_DIR = \"D:/cbers_data/DataSetModelo/CALIBRATING_MODEL/Training/\"\n",
    "SUBSCENES_DIR = \"D:/cbers_data/DataSetModelo/CALIBRATING_MODEL/Subscenes/\"\n",
    "SUBMASKS_DIR = \"D:/cbers_data/DataSetModelo/CALIBRATING_MODEL/Submasks/\"\n",
    "NORMALIZED_DIR = \"D:/cbers_data/DataSetModelo/CALIBRATING_MODEL/Normalized_scenes/\"\n",
    "VRT_DATA_DIR = \"D:/cbers_data/DataSetModelo/CALIBRATING_MODEL/vrt/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_img(image,x,y,z):\n",
    "    image = cv2.resize(image,(x,y))\n",
    "    image = image.reshape((x,y,z))\n",
    "    return image\n",
    "\n",
    "def print_cbers_image(image):\n",
    "    plt.figure()\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIMENSION = 256\n",
    "BANDS = 4\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre processing the raw imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.handle_data import create_training_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create training folders\n",
    "create_training_folders(TRAINING_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform raw data to clipped image (Necessário quando as imagens estão separadas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all files from raw_data\n",
    "FILES = os.listdir(SCENES_DIR)\n",
    "#get name of files\n",
    "FILES = [i.split('_BAND', 1)[0] for i in FILES]\n",
    "#remove duplicateds\n",
    "FILES = list(dict.fromkeys(FILES))\n",
    "#Clip All images first\n",
    "for image in FILES:\n",
    "    clip_image(SCENES_DIR,CLIPPED_DATA_DIR,image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize Scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_SCENES_LIST = os.listdir(SCENES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.normalize import normalize_scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_scene(SCENES_DIR,RAW_SCENES_LIST,NORMALIZED_DIR, BANDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crop Scene/MASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.handle_data import crop_scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop normalized scene\n",
    "crop_scene(NORMALIZED_DIR,SUBSCENES_DIR,DIMENSION)\n",
    "# crop mask\n",
    "crop_scene(MASKS_DIR,SUBMASKS_DIR,DIMENSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.split_dataset import split_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset(SUBSCENES_DIR,SUBMASKS_DIR,TRAINING_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import random\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frame_path = 'D:/cbers_data/DataSetModelo/CALIBRATING_MODEL/Training/train_frames'\n",
    "train_mask_path = 'D:/cbers_data/DataSetModelo/CALIBRATING_MODEL/Training/train_masks'\n",
    "\n",
    "val_frame_path = 'D:/cbers_data/DataSetModelo/CALIBRATING_MODEL/Training/val_frames'\n",
    "val_mask_path = 'D:/cbers_data/DataSetModelo/CALIBRATING_MODEL/Training/val_masks'\n",
    "\n",
    "test_frame_path = 'D:/cbers_data/DataSetModelo/CALIBRATING_MODEL/Training/test_frames'\n",
    "test_mask_path = 'D:/cbers_data/DataSetModelo/CALIBRATING_MODEL/Training/test_masks'\n",
    "\n",
    "weights_path = 'D:/cbers_data/DataSetModelo/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.generator import data_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = data_gen(train_frame_path,train_mask_path, batch_size = BATCH_SIZE,DIMENSION,BANDS)\n",
    "val_gen = data_gen(val_frame_path,val_mask_path, batch_size = BATCH_SIZE,DIMENSION,BANDS)\n",
    "test_gen = data_gen(test_frame_path,test_mask_path, batch_size = BATCH_SIZE,DIMENSION,BANDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import visualkeras\n",
    "from tensorflow.python.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, ZeroPadding2D\n",
    "from collections import defaultdict\n",
    "from utils.f1_score import f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_unet.models import satellite_unet\n",
    "model = satellite_unet(input_shape=(DIMENSION, DIMENSION, 4))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[\"accuracy\",f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=1328x207 at 0x19C84A15730>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualkeras.layered_view(model,legend=True, scale_xy=0.5, scale_z=0.5, max_z=7, to_file='network.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model_hyperparameters import config_hyper_parameters\n",
    "callback_list = config_hyper_parameters(weights_path, monitor='f1', mode='max', min_delta=0.01, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DIMENSION' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-4694e1049dd0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#training the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m model_history = model.fit(train_gen, \n\u001b[0m\u001b[0;32m      6\u001b[0m                     \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                     \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ev_geo\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ev_geo\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1047\u001b[0m          \u001b[0mtraining_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRespectCompiledTrainableState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m       \u001b[1;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1049\u001b[1;33m       data_handler = data_adapter.DataHandler(\n\u001b[0m\u001b[0;32m   1050\u001b[0m           \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m           \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ev_geo\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[0;32m   1103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1105\u001b[1;33m     self._adapter = adapter_cls(\n\u001b[0m\u001b[0;32m   1106\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ev_geo\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[0;32m    784\u001b[0m     \u001b[1;31m# Since we have to know the dtype of the python generator when we build the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m     \u001b[1;31m# dataset, we have to look at a batch to infer the structure.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m     \u001b[0mpeek\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_peek_and_restore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    787\u001b[0m     \u001b[0mpeek\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_standardize_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m     \u001b[0mpeek\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ev_geo\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m_peek_and_restore\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    841\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_peek_and_restore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 843\u001b[1;33m     \u001b[0mpeek\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    844\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpeek\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpeek\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\TCC\\Cloud-segmentation\\utils\\generator.py\u001b[0m in \u001b[0;36mdata_gen\u001b[1;34m(img_folder, mask_folder, batch_size)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mwhile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDIMENSION\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDIMENSION\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBANDS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDIMENSION\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDIMENSION\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DIMENSION' is not defined"
     ]
    }
   ],
   "source": [
    "NO_OF_TRAINING_IMAGES = len(os.listdir(train_frame_path))\n",
    "NO_OF_VAL_IMAGES = len(os.listdir(val_frame_path))\n",
    "\n",
    "#training the model\n",
    "model_history = model.fit(train_gen, \n",
    "                    epochs = 15, \n",
    "                    batch_size = BATCH_SIZE, \n",
    "                    validation_data = val_gen, \n",
    "                    callbacks = callback_list,\n",
    "                    steps_per_epoch =(NO_OF_TRAINING_IMAGES/BATCH_SIZE), \n",
    "                    validation_steps = (NO_OF_VAL_IMAGES/BATCH_SIZE)\n",
    "                  )\n",
    "\n",
    "model.save('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import PIL\n",
    "from PIL import ImageOps\n",
    "\n",
    "\n",
    "def get_test_batch(qnt_images):\n",
    "    evaluate = []\n",
    "    \n",
    "    convert_to_image = keras.preprocessing.image.array_to_img\n",
    "    dir_images = test_frame_path\n",
    "    dir_masks = test_mask_path\n",
    "    test_files = os.listdir(test_frame_path)\n",
    "    #batch_images_structure\n",
    "    mask_true_image_test = np.zeros((qnt_images,DIMENSION,DIMENSION,1))\n",
    "    batch_test_images = np.zeros((qnt_images,DIMENSION,DIMENSION,3))\n",
    "\n",
    "    for i in range(qnt_images):\n",
    "        mask_test = gdal.Open(dir_masks + '/'+ test_files[i]).ReadAsArray()\n",
    "        mask_test = cv2.normalize(mask_test, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "        mask_test = np.expand_dims(mask_test, axis=2)\n",
    "\n",
    "        imagem_teste = gdal.Open(dir_images + '/'+ test_files[i]).ReadAsArray()\n",
    "        imagem_teste = imagem_teste.transpose((1,2,0))\n",
    "        imagem_teste = cv2.normalize(imagem_teste, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "\n",
    "        \n",
    "        y_eval = np.expand_dims(mask_test, axis=0)\n",
    "        x_eval = np.expand_dims(imagem_teste, axis=0)\n",
    "        \n",
    "        evaluate.append(model.evaluate(x=x_eval,y=y_eval))\n",
    "        \n",
    "        mask_true_image_test [i,:,:] = mask_test\n",
    "        batch_test_images [i,:,:] = imagem_teste\n",
    "    return batch_test_images, mask_true_image_test, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnt_images = 500\n",
    "batch_test_images, mask_true_image_test, evaluate = get_test_batch(qnt_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_eval = pd.DataFrame(evaluate)\n",
    "df_eval.columns = [\"Loss\", \"Accuracy\", \"f1\"]\n",
    "df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( 'Loss: ' + str(df_eval['Loss'].mean()))\n",
    "print( 'F1: ' + str(df_eval['f1'].mean()))\n",
    "print( 'Accuracy: ' + str(df_eval['Accuracy'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_predict = model.predict(batch_test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(display_list):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    title = ['Input Image', 'CMask', 'Predicted Mask']\n",
    "\n",
    "    \n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i+1)\n",
    "        plt.title(title[i])\n",
    "        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
    "        plt.axis('off')\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(qnt_images):\n",
    "    b, g, r  = batch_test_images[i][:, :, 0], batch_test_images[i][:, :, 1], batch_test_images[i][:, :, 2]\n",
    "    rgb = np.dstack((r,g,b))\n",
    "    show_images([rgb, mask_true_image_test[i], mask_predict[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc = model_history.history['accuracy']\n",
    "val_acc = model_history.history['val_accuracy']\n",
    "\n",
    "loss = model_history.history['loss']\n",
    "val_loss = model_history.history['val_loss']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model_history.history['loss']\n",
    "val_loss = model_history.history['val_loss']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = model_history.history['f1']\n",
    "val_f1 = model_history.history['val_f1']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(f1, label='Training F1-Score')\n",
    "plt.plot(val_f1, label='Validation F1-Score')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('F1-Score')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation F1-Score')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "croped = 'D:/cbers_data/DataSetModelo/train_frames/'\n",
    "list_image = glob.glob(croped + \"CBERS_4A_WFI_20201020_205_116_L4_*_*_*_.tif\")\n",
    "sorted(list_image, key=lambda x: int(x.split('_')[11]))\n",
    "g = gdal.Warp(\"output.tif\", list_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELO JA TREINADO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recarregamento do modelo treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import sklearn\n",
    "import random\n",
    "from osgeo import gdal\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"D:/cbers_data/DataSetModelo/\"\n",
    "RAW_DATA_DIR = \"D:/cbers_data/DataSetModelo/raw_scene/\"\n",
    "RAW_MASK_DIR = \"D:/cbers_data/DataSetModelo/raw_mask/\"\n",
    "CLIPPED_DATA_DIR = \"D:/cbers_data/DataSetModelo/clip/\"\n",
    "VRT_DATA_DIR = \"D:/cbers_data/DataSetModelo/vrt/\"\n",
    "PREDICT_DIR = \"D:/cbers_data/DataSetModelo/predict/\"\n",
    "DIMENSION = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_min_max(image):\n",
    "    \n",
    "    return((image-np.nanmin(image))/(np.nanmax(image)- np.nanmin(image)))\n",
    "\n",
    "def resize_img(image,x,y,z):\n",
    "    image = cv2.resize(image,(x,y))\n",
    "    image = image.reshape((x,y,z))\n",
    "    return image\n",
    "\n",
    "def print_cbers_image(image):\n",
    "    plt.figure()\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_image(input_path, output_path,image_name):\n",
    "    red = gdal.Open(input_path + image_name + '_BAND13_GRID_SURFACE.tif')\n",
    "    green = gdal.Open(input_path + image_name + '_BAND14_GRID_SURFACE.tif')\n",
    "    blue = gdal.Open(input_path + image_name + '_BAND15_GRID_SURFACE.tif')\n",
    "    #nir = gdal.Open(input_path + image_name + '_BAND16_GRID_SURFACE.tif')\n",
    "    #Create the .vrt from RGBN\n",
    "    #array = [red, green, blue,/nir]#adicionar o nir\n",
    "    array = [red, green, blue]#adicionar o nir\n",
    "    opt = gdal.BuildVRTOptions(srcNodata=-9999, VRTNodata=-9999,separate=True,resampleAlg='nearest')\n",
    "    vrt_clip = gdal.BuildVRT(VRT_DATA_DIR + image_name +'.vrt', array, options=opt)\n",
    "    #Translate the .vrt to .tif\n",
    "    trans_opt = gdal.TranslateOptions(format=\"tif\", outputType=gdal.gdalconst.GDT_Unknown, \n",
    "                                  bandList=[1,2,3],width=0, height=0, widthPct=0.0, \n",
    "                                  heightPct=0.0, xRes=0.0, yRes=0.0,noData=-9999)\n",
    "    #Clip da imagem\n",
    "    gdal.Translate(output_path + image_name +'_rgbn.tif', vrt_clip)\n",
    "    #Apaga o vrt\n",
    "    #os.remove(image_name + \".vrt\") \n",
    "    return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image_rgbn(output_dir, rgbn, image_name, dimension):\n",
    "    gt = rgbn.GetGeoTransform()\n",
    "    x_min = gt[0]\n",
    "    y_max = gt[3]\n",
    "\n",
    "    res = gt[1]\n",
    "\n",
    "    num_img_x = rgbn.RasterXSize/dimension\n",
    "    num_img_y = rgbn.RasterYSize/dimension\n",
    "\n",
    "    x_len = res * rgbn.RasterXSize\n",
    "    y_len = res * rgbn.RasterYSize\n",
    "\n",
    "    x_size = x_len/num_img_x\n",
    "    y_size = y_len/num_img_y\n",
    "\n",
    "\n",
    "    x_steps = [x_min + x_size * i for i in range(int(num_img_x) + 1)]\n",
    "    y_steps = [y_max - y_size * i for i in range(int(num_img_y) + 1)]\n",
    "    print(\"qnt img x: \" + str(num_img_x))\n",
    "    print(\"qnt img y: \" + str(num_img_y))\n",
    "    index_img = 0\n",
    "    for i in range(int(num_img_x)):\n",
    "        for j in range(int(num_img_y)):\n",
    "            x_min = x_steps[i]\n",
    "            x_max = x_steps[i+1]\n",
    "            y_max = y_steps[j]\n",
    "            y_min = y_steps[j+1]\n",
    "            index_img+=1\n",
    "            \n",
    "            gdal.Warp(output_dir + image_name+ \"_\" + str(i)+ \"_\" +str(j)+'_'+str(index_img)+\"_\"+\".tif\", rgbn, \n",
    "                      outputBounds = (x_min,y_min,x_max, y_max), dstNodata = -9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "TESTE_FRAME_DIR = 'D:/cbers_data/DataSetModelo/test_frames/'\n",
    "TESTE_MASK_DIR = 'D:/cbers_data/DataSetModelo/test_masks/'\n",
    "# Load image\n",
    "\n",
    "test_frames_list = glob.glob(TESTE_FRAME_DIR + \"CBERS_4A_WFI_20201025_235_108_L4_*_*_*_.tif\")\n",
    "test_masks_list = glob.glob(TESTE_MASK_DIR + \"CBERS_4A_WFI_20201025_235_108_L4_*_*_*_.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QNT_SUB_FRAMES = len(test_frames_list)\n",
    "test_images = np.zeros((QNT_SUB_FRAMES,DIMENSION,DIMENSION,3))\n",
    "\n",
    "for i, ele in enumerate(test_frames_list):\n",
    "    image_test = gdal.Open(ele).ReadAsArray()\n",
    "    image_test = image_test.transpose((1,2,0))\n",
    "    image_test = cv2.normalize(image_test, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "    test_images [i,:,:] = image_test\n",
    "    plt.imshow(tf.keras.preprocessing.image.array_to_img(image_test))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRGB(template,arr,filename):\n",
    "    '''Creates a copy of a 3-band raster with values from array\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        template: Path to template raster\n",
    "        arr: Value array with dimensions (r,c,3)\n",
    "        filename: Output filename for new raster \n",
    "    '''\n",
    "\n",
    "    # Open template\n",
    "    t = gdal.Open(template)\n",
    "\n",
    "    # Get geotiff driver\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "\n",
    "    # Create new raster\n",
    "    r = driver.Create(filename, t.RasterXSize, t.RasterYSize, 3, gdal.GDT_Byte,['COMPRESS=LZW'])\n",
    "\n",
    "    # Set metadata\n",
    "    r.SetGeoTransform(t.GetGeoTransform())\n",
    "    r.SetProjection(t.GetProjection())\n",
    "\n",
    "    # loop through bands and write new values\n",
    "    for bix in range(3):\n",
    "\n",
    "        rb = r.GetRasterBand(bix+1)\n",
    "\n",
    "        # Write array\n",
    "        rb.WriteArray(arr[...,bix])\n",
    "\n",
    "    # Close datasets\n",
    "    t = None\n",
    "    r = None\n",
    "    rb = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('model', custom_objects={'f1':f1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicts = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MOSTRA AS MASCARAS DAS IMAGENS\n",
    "def show_mask(test_predicts):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    title = 'Predicted Mask'\n",
    "    plt.title(title)\n",
    "    plt.imshow(tf.keras.preprocessing.image.array_to_img(test_predicts))\n",
    "    plt.axis('off')\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRANSFORMA MASCARA OUTPUT EM RASTER\n",
    "def array2raster(newRasterfn,rasterOrigin,pixelWidth,pixelHeight,array):\n",
    "\n",
    "    cols = array.shape[1]\n",
    "    rows = array.shape[0]\n",
    "    originX = rasterOrigin[0]\n",
    "    originY = rasterOrigin[1]\n",
    "\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    outRaster = driver.Create(newRasterfn, cols, rows, 1, gdal.GDT_Byte)\n",
    "    outRaster.SetGeoTransform((originX, pixelWidth, 0, originY, 0, pixelHeight))\n",
    "    outband = outRaster.GetRasterBand(1)\n",
    "    outband.WriteArray(array)\n",
    "    outRasterSRS = osr.SpatialReference()\n",
    "    outRasterSRS.ImportFromEPSG(4326)\n",
    "    outRaster.SetProjection(outRasterSRS.ExportToWkt())\n",
    "    outband.FlushCache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, file_input in enumerate(test_frames_list):\n",
    "    file_output = file_input.replace('test_frames','predict')\n",
    "    aux = gdal.Open(file_input)\n",
    "    geo_t = aux.GetGeoTransform()\n",
    "    origin = (geo_t[0],geo_t[3])\n",
    "    px_width = geo_t[1]\n",
    "    px_height = geo_t[5]\n",
    "    array = test_predicts[i,:, :, 0]\n",
    "    array2raster(file_output,origin,px_width,px_height,array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i, file_input in enumerate(test_frames_list):\n",
    "    file_output = file_input.replace('test_frames','predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnt_images = 500\n",
    "for i in range(qnt_images):\n",
    "    b, g, r  = batch_test_images[i][:, :, 0], batch_test_images[i][:, :, 1], batch_test_images[i][:, :, 2]\n",
    "    rgb = np.dstack((r,g,b))\n",
    "    show_images([rgb, mask_true_image_test[i], mask_predict[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    show_images([test_predicts[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_predicts[321]*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste = gdal.Open('D:\\\\cbers_data\\\\DataSetModelo\\\\test_masks\\\\CBERS_4A_WFI_20201020_205_116_L4_0_51_52_.tif').ReadAsArray()\n",
    "teste = np.expand_dims(teste,2)\n",
    "show_mask(teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dimensoes subscenes\n",
    "DIMENSION = 256\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CAMINHO DOS ARQUIVOS DE ENTRADA (IMAGEM COMPLETA) (ENTRADA)\n",
    "production_frame_path = \"D:/cbers_data/DataSetModelo/ProductionFrames/\"\n",
    "\n",
    "#CAMINHO QUE ARMAZENARÁ AS SUBCENAS DA IMAGEM DE ENTRADA (SAIDA)\n",
    "production_cropped_frame_path = \"D:/cbers_data/DataSetModelo/ProductionCroppedFrames/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função responsável por cortar as imagens de entrada e salvá-las num diretório de saída\n",
    "def save_cropped_production_images(input_path_base, output_path_base, DIMENSION):\n",
    "    #Pega lista de imagens a serem cortadas\n",
    "    raw_images_list_name = os.listdir(production_frame_path)\n",
    "    for frame_name in raw_images_list_name:\n",
    "        #Pasta de saída terá mesmo nome do arquivo de entrada (porém sem o .tif)\n",
    "        output_path = output_path_base + frame_name.split('.')[0]+\"/\"\n",
    "        #Caminho dos arquivos de entrada (imagens cruas)\n",
    "        input_path = input_path_base + frame_name\n",
    "        #Abre a imagem de entrada\n",
    "        raw_scene = gdal.Open(input_path)\n",
    "        #Cria uma pasta com o nome do arquivo de entrada\n",
    "        if not os.path.isdir(output_path):\n",
    "            os.mkdir(output_path)\n",
    "        #Corta as imagens de entrada e as salva no diretório de saída\n",
    "        crop_image_rgbn(output_path, raw_scene, frame_name.split(\".\")[0], DIMENSION)\n",
    "        \n",
    "#Chamada da função\n",
    "save_cropped_production_images(production_frame_path,production_cropped_frame_path, DIMENSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LISTA QUE CONTEM O CAMINHO DE CADA SUBCENA GERADA (ENTRADAS)\n",
    "subscenes_path_list = [production_cropped_frame_path + file_name + \"/\" for file_name in os.listdir(production_cropped_frame_path)]\n",
    "\n",
    "#CAMINHO QUE ARMAZENARÁ AS MASCARAS DAS SUBCENAS (SAIDA)\n",
    "production_cropped_mask_path = \"D:/cbers_data/DataSetModelo/ProductionCroppedMasks/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função responsável por chamar o modelo para gerar as mascaras e a salvar no diretório de saída\n",
    "def save_production_cropped_masks(input_dir_list, output_path):\n",
    "    #FOR que caminha entre os diretórios das subcenas\n",
    "    for i, dir_name in enumerate(input_dir_list):\n",
    "        #FOR que entrou no diretório de uma subcenas e caminha por elas\n",
    "        input_file_list = os.listdir(dir_name)\n",
    "        for j, file_name in enumerate(input_file_list):\n",
    "            subscene = gdal.Open(dir_name+file_name)\n",
    "            subscene_array = subscene.ReadAsArray().transpose((1,2,0))\n",
    "            subscene_normalized =  cv2.normalize(subscene_array, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "            #Dimensões são adequadas para o predict do modelo\n",
    "            subscene_expanded = np.expand_dims(subscene_normalized, axis=0)\n",
    "            subscene_mask = model.predict(subscene_expanded)\n",
    "            \n",
    "            ##Etapa para configurar o .tif das mascaras\n",
    "            driver = gdal.GetDriverByName(\"GTiff\")\n",
    "            metadata = driver.GetMetadata()\n",
    "            \n",
    "            #Transforma shape da mascara de (1,256,256,1) para (256,256)\n",
    "            subscene_mask = subscene_mask.squeeze()\n",
    "            #NOME DA CENA ORIGINAL (IMAGEM INTEIRA)\n",
    "            original_scene_name = file_name.split(\"rgbn\")[0]+\"rgbn\"\n",
    "            #Cria uma pasta com o nome do arquivo original (IMAGEM COMPLETA)\n",
    "            if not os.path.isdir(output_path + original_scene_name):\n",
    "                os.mkdir(output_path + original_scene_name)\n",
    "            #Cria um raster (.tif) da mascara com o mesmo tamanho da subscena\n",
    "            subscene_mask_raster = driver.Create(output_path + original_scene_name + \"/\" + file_name,\n",
    "                                subscene.RasterXSize,\n",
    "                                subscene.RasterYSize,\n",
    "                                1, #Numero de bandas da mascara\n",
    "                                gdal.GDT_Float32)\n",
    "            #COPIA AS PROJEÇÕES DA SUBSCENA\n",
    "            subscene_mask_raster.SetProjection(subscene.GetProjectionRef())\n",
    "            subscene_mask_raster.SetGeoTransform(subscene.GetGeoTransform()) \n",
    "            \n",
    "            #Get the band to write to\n",
    "            #out_band = subscene.GetRasterBand(1)\n",
    "            #ESCREVE A MASCARA EM DISCO\n",
    "            subscene_mask_raster.WriteArray(subscene_mask)\n",
    "            \n",
    "save_production_cropped_masks(subscenes_path_list, production_cropped_mask_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LISTA QUE CONTEM O CAMINHO DE CADA MASCARA GERADA (ENTRADAS)\n",
    "masks_path_list = [production_cropped_mask_path + file_name + \"/\" for file_name in os.listdir(production_cropped_mask_path)]\n",
    "\n",
    "\n",
    "# Caminho para o diretório onde deverá ser salvo a máscara após o mosaico (SAIDA)\n",
    "mask_mosaic_path = \"D:/cbers_data/DataSetModelo/MergeFile/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, folder_name in enumerate(masks_path_list):\n",
    "    folder_input_list = os.listdir(folder_name)\n",
    "    files_to_mosaic_path = [folder_name + file_name  for file_name in folder_input_list]\n",
    "    print(folder_input_list[0].split('rgbn')[0] + \"rgbn/\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import osgeo_utils.gdal_merge\n",
    "def mount_mosaic_of_masks(input_path, output_path):\n",
    "    for i, folder_name in enumerate(input_path):\n",
    "        files_to_mosaic = os.listdir(folder_name)\n",
    "        files_to_mosaic_path = [folder_name + file_name  for file_name in files_to_mosaic]\n",
    "        #Cria uma pasta com o nome do arquivo original (IMAGEM COMPLETA)\n",
    "        original_file_name = files_to_mosaic[0].split(\"rgbn\")[0]+\"rgbn\"\n",
    "        if not os.path.isdir(output_path + original_file_name):\n",
    "            os.mkdir(output_path + original_file_name)\n",
    "        #ESCREVE EM DISCO\n",
    "        print(output_path + original_file_name + \"/\" + original_file_name + \n",
    "                      \".tif\")\n",
    "        g = gdal.Warp(output_path + original_file_name + \"/\" + original_file_name + \n",
    "                      \".tif\", files_to_mosaic_path, format=\"GTiff\")\n",
    "        g = None # Close file and flush to disk\n",
    "        \n",
    "mount_mosaic_of_masks(masks_path_list, mask_mosaic_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
