{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import sklearn\n",
    "import random\n",
    "from osgeo import gdal\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"D:/cbers_data/DataSetModelo/\"\n",
    "\n",
    "SCENES_PATH = \"D:/cbers_data/DataSetModelo/raw_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIMENSION = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_min_max(image):\n",
    "    \n",
    "    return((image-np.nanmin(image))/(np.nanmax(image)- np.nanmin(image)))\n",
    "\n",
    "def resize_img(image,x,y,z):\n",
    "    image = cv2.resize(image,(x,y))\n",
    "    image = image.reshape((x,y,z))\n",
    "    return image\n",
    "\n",
    "#def clip_image(r,g,b,n):\n",
    "#    return np.dstack((r,g,b,n))\n",
    "\n",
    "def print_cbers_image(image):\n",
    "    plt.figure()\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre processing the raw imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating folders\n",
    "\n",
    "try:\n",
    "    folders = ['train_frames', 'train_masks', 'val_frames', 'val_masks', 'test_frames', 'test_masks']\n",
    "    for folder in folders:\n",
    "        os.makedirs(DATA_PATH + folder)\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating folders\n",
    "try:\n",
    "    folders = ['train_frames', 'train_masks', 'val_frames', 'val_masks', \n",
    "               'test_frames', 'test_masks']\n",
    "    for folder in folders:\n",
    "        os.makedirs(DATA_PATH + folder)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILES = ['CBERS_4A_WFI_20201020_205_116_L4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split dataset files\n",
    "all_frames = FILES\n",
    "random.seed(4)\n",
    "random.shuffle(all_frames)\n",
    "\n",
    "train_size = int(0.7 * len(all_frames))\n",
    "val_size = int(0.15 * len(all_frames))\n",
    "test_size = int(0.15 * len(all_frames))\n",
    "\n",
    "train_scene = all_frames[:train_size]\n",
    "val_scene = all_frames[train_size:train_size+val_size]\n",
    "test_scene = val_subscenes = all_frames[train_size+val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate corresponding mask lists for masks\n",
    "train_masks = [f for f in all_frames if f in train_scene]\n",
    "val_masks = [f for f in all_frames if f in val_scene]\n",
    "test_masks = [f for f in all_frames if f in test_scene]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_image(input_path, output_path,image_name):\n",
    "    \n",
    "    red = gdal.Open(input_path + image_name + '_BAND13_GRID_SURFACE.tif')\n",
    "    green = gdal.Open(input_path + image_name + '_BAND14_GRID_SURFACE.tif')\n",
    "    blue = gdal.Open(input_path + image_name + '_BAND15_GRID_SURFACE.tif')\n",
    "    #nir = gdal.Open(input_path + image_name + '_BAND16_GRID_SURFACE.tif')\n",
    "    #Create the .vrt from RGBN\n",
    "    array = [red, green, blue]#adicionar o nir\n",
    "    opt = gdal.BuildVRTOptions(srcNodata=-9999, VRTNodata=-9999,separate=True,resampleAlg='nearest')\n",
    "    vrt_clip = gdal.BuildVRT(input_path + image_name +'.vrt', array, options=opt)\n",
    "    #Translate the .vrt to .tif\n",
    "    trans_opt = gdal.TranslateOptions(format=\"tif\", outputType=gdal.gdalconst.GDT_Unknown, \n",
    "                                  bandList=[1,2,3],width=0, height=0, widthPct=0.0, \n",
    "                                  heightPct=0.0, xRes=0.0, yRes=0.0,noData=-9999)\n",
    "    \n",
    "    return gdal.Translate(output_path + image_name +'_rgbn.tif', vrt_clip)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def crop_image_rgbn(output_dir, rgbn, image_name, dimension):\n",
    "    gt = rgbn.GetGeoTransform()\n",
    "    x_min = gt[0]\n",
    "    y_max = gt[3]\n",
    "\n",
    "    res = gt[1]\n",
    "\n",
    "    num_img_x = rgbn.RasterXSize/dimension\n",
    "    num_img_y = rgbn.RasterYSize/dimension\n",
    "\n",
    "    x_len = res * rgbn.RasterXSize\n",
    "    y_len = res * rgbn.RasterYSize\n",
    "\n",
    "    x_size = x_len/num_img_x\n",
    "    y_size = y_len/num_img_y\n",
    "\n",
    "\n",
    "    x_steps = [x_min + x_size * i for i in range(int(num_img_x) + 1)]\n",
    "    y_steps = [y_max - y_size * i for i in range(int(num_img_y) + 1)]\n",
    "    print(\"qnt img: \" + str(num_img_y))\n",
    "\n",
    "    for i in range(int(min(num_img_x,num_img_y))):\n",
    "        for j in range(int(min(num_img_x,num_img_y))):\n",
    "            x_min = x_steps[i]\n",
    "            x_max = x_steps[i+1]\n",
    "            y_max = y_steps[j]\n",
    "            y_min = y_steps[j+1]\n",
    "\n",
    "            gdal.Warp(output_dir + image_name + str(i)+ \"_\" +str(j)+'.tif', rgbn, \n",
    "                      outputBounds = (x_min,y_min,x_max, y_max), dstNodata = -9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(normalize_min_max(first_im.transpose((1,2,0))))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add train, val frames and masks to relevant folders\n",
    "from numpy import save\n",
    "\n",
    "def add_frames(dir_name, image_name):\n",
    "    try:\n",
    "        dir_name = \"D:/cbers_data/DataSetModelo/\" + dir_name + '/'\n",
    "        #clip Image\n",
    "        raw_path = \"D:/cbers_data/DataSetModelo/raw_data/\" \n",
    "        clip_path = \"D:/cbers_data/DataSetModelo/clip/\" \n",
    "        clip_img = clip_image(raw_path, clip_path,image_name)\n",
    "        #crop image\n",
    "        crop_image_rgbn(dir_name, clip_img, image_name, DIMENSION)\n",
    "        \n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        pass\n",
    "    \n",
    "def add_masks(dir_name, image_name):\n",
    "    try:\n",
    "        dir_name = \"D:/cbers_data/DataSetModelo/\" + dir_name + '/'\n",
    "        #crop image\n",
    "        mask = gdal.Open(\"D:/cbers_data/DataSetModelo/raw_data/\" + image_name + '_CMASK_GRID_SURFACE.tif')\n",
    "        crop_image_rgbn(dir_name, mask, image_name, DIMENSION)\n",
    " \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_folders = [(train_scene, 'train_frames'), (val_scene, 'val_frames'), (test_scene, 'test_frames')]\n",
    "\n",
    "mask_folders = [(train_masks, 'train_masks'), (val_masks, 'val_masks'), (test_masks, 'test_masks')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Celula que carrega as imagens nos diretórios corretos\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Add frames\n",
    "for folder in frame_folders:\n",
    "    print(folder)\n",
    "    array = folder[0]\n",
    "    name = [folder[1]] * len(array)\n",
    "    list(map(add_frames, name, array))\n",
    "\n",
    "# Add masks\n",
    "for folder in mask_folders:\n",
    "    print(folder)\n",
    "    array = folder[0]\n",
    "    name = [folder[1]] * len(array)\n",
    "    list(map(add_masks, name, array))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        data_format=None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images belonging to 0 classes.\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 4\n",
    "\n",
    "train_image_generator = train_datagen.flow_from_directory(\n",
    "'D:/Geoprocessamento/DataSetModelo/l8_biome/train_frames',\n",
    "batch_size = BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import random\n",
    "from sklearn.model_selection import ParameterGrid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "def data_gen(img_folder, mask_folder, batch_size):\n",
    "    c = 0\n",
    "    n = os.listdir(img_folder) #List of training images\n",
    "    random.shuffle(n)\n",
    "    x,y,z = 256,256,1\n",
    "    while(True):\n",
    "        img = np.zeros((batch_size, 256, 256, 3)).astype('float')\n",
    "        mask = np.zeros((batch_size, 256, 256, 1)).astype('float')\n",
    "\n",
    "        for i in range(c, c + batch_size): #initially from 0 to 16, c = 0. \n",
    "            #train_img = np.load(img_folder+'/'+n[i], allow_pickle=True)\n",
    "            train_img = gdal.Open(img_folder+'/'+n[i]).ReadAsArray()\n",
    "\n",
    "            #train_img =  cv2.resize(train_img, (256, 256))# Read an image from folder and resize\n",
    "            #train_img = train_img.reshape(256, 256, 4)\n",
    "            #Normalização\n",
    "            train_img = cv2.normalize(train_img, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "\n",
    "            #train_img = normalize_min_max(train_img)\n",
    "            train_img = tf.convert_to_tensor(train_img.transpose((1,2,0)))\n",
    "            img[i-c] = train_img #add to array - img[0], img[1], and so on.\n",
    "\n",
    "            #n[i] = n[i][:-3] + \"tif\"\n",
    "            \n",
    "            train_mask = gdal.Open(mask_folder+'/'+n[i]).ReadAsArray()\n",
    "            #train_mask = (np.load(mask_folder+'/'+n[i]))\n",
    "            train_mask = train_mask/1\n",
    "            #train_mask = cv2.resize(train_mask, (256, 256))\n",
    "            train_mask = resize_img(train_mask,x,y,z)\n",
    "            #train_mask = normalize_min_max(train_mask)\n",
    "            train_mask = cv2.normalize(train_mask, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "            #train_mask = np.expand_dims(train_mask,axis=2)\n",
    "            train_mask = np.expand_dims(train_mask, axis=2)\n",
    "            train_mask = tf.convert_to_tensor(train_mask)\n",
    "            \n",
    "            mask[i-c] = train_mask\n",
    "            \n",
    "\n",
    "        c += batch_size\n",
    "        \n",
    "        if(c + batch_size >= len(os.listdir(img_folder))):\n",
    "            c=0\n",
    "            random.shuffle(n)\n",
    "                      # print \"randomizing again\"\n",
    "        \n",
    "        yield img, mask\n",
    "        \n",
    "train_frame_path = 'D:/cbers_data/DataSetModelo/train_frames'\n",
    "train_mask_path = 'D:/cbers_data/DataSetModelo/train_masks'\n",
    "\n",
    "val_frame_path = 'D:/cbers_data/DataSetModelo/val_frames'\n",
    "val_mask_path = 'D:/cbers_data/DataSetModelo/val_masks'\n",
    "\n",
    "test_frame_path = 'D:/cbers_data/DataSetModelo/test_frames'\n",
    "test_mask_path = 'D:/cbers_data/DataSetModelo/test_masks'\n",
    "\n",
    "train_gen = data_gen(train_frame_path,train_mask_path, batch_size = BATCH_SIZE)\n",
    "val_gen = data_gen(val_frame_path,val_mask_path, batch_size = BATCH_SIZE)\n",
    "test_gen = data_gen(test_frame_path,test_mask_path, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "NO_OF_TRAINING_IMAGES = len(os.listdir(train_frame_path))\n",
    "NO_OF_VAL_IMAGES = len(os.listdir(val_frame_path))\n",
    "weights_path = 'D:/cbers_data/DataSetModelo/'\n",
    "\n",
    "checkpoint = ModelCheckpoint(weights_path, monitor='f1', \n",
    "                             verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "csv_logger = CSVLogger('D:/cbers_data/DataSetModelo/log.out', append=True, separator=';')\n",
    "\n",
    "earlystopping = EarlyStopping(monitor = 'f1', verbose = 1,\n",
    "                              min_delta = 0.01, patience = 20, mode = 'max')\n",
    "\n",
    "callbacks_list = [checkpoint, csv_logger, earlystopping]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from itertools import chain\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input\n",
    "from keras.layers.core import Dropout, Lambda\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 256, 256, 32) 896         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256, 256, 32) 0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 256, 256, 32) 9248        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 128, 128, 32) 0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 128, 128, 32) 9248        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128, 128, 32) 0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 128, 128, 32) 9248        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 32)   0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 64, 64, 64)   18496       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 64, 64, 64)   0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 64, 64, 64)   36928       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 64)   0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 256)  147712      max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 32, 32, 256)  0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 256)  590080      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 256)  0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 128)  295040      max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 16, 16, 128)  0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 128)  147584      dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 32, 32, 256)  131328      conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 32, 32, 512)  0           conv2d_transpose[0][0]           \n",
      "                                                                 conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 256)  1179904     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 32, 32, 256)  0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 256)  590080      dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 64, 64, 64)   65600       conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 64, 64, 128)  0           conv2d_transpose_1[0][0]         \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 64, 64, 64)   73792       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 64, 64, 64)   0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 64, 64, 64)   36928       dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 128, 128, 32) 8224        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 128, 128, 64) 0           conv2d_transpose_2[0][0]         \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 128, 128, 32) 18464       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 128, 128, 32) 0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 128, 128, 32) 9248        dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 256, 256, 32) 4128        conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 256, 256, 64) 0           conv2d_transpose_3[0][0]         \n",
      "                                                                 conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 256, 256, 32) 18464       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 256, 256, 32) 0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 256, 256, 32) 9248        dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 256, 256, 1)  33          conv2d_17[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,409,921\n",
      "Trainable params: 3,409,921\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build U-Net model\n",
    "\n",
    "inputs = Input((256, 256, 3))\n",
    "s = (inputs)\n",
    "\n",
    "c1 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (s)\n",
    "c1 = Dropout(0.1) (c1)\n",
    "c1 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c1)\n",
    "p1 = MaxPooling2D((2, 2)) (c1)\n",
    "\n",
    "c2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p1)\n",
    "c2 = Dropout(0.1) (c2)\n",
    "c2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c2)\n",
    "p2 = MaxPooling2D((2, 2)) (c2)\n",
    "\n",
    "c3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p2)\n",
    "c3 = Dropout(0.2) (c3)\n",
    "c3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c3)\n",
    "p3 = MaxPooling2D((2, 2)) (c3)\n",
    "\n",
    "c4 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p3)\n",
    "c4 = Dropout(0.2) (c4)\n",
    "c4 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c4)\n",
    "p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
    "\n",
    "c5 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p4)\n",
    "c5 = Dropout(0.3) (c5)\n",
    "c5 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c5)\n",
    "\n",
    "u6 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same') (c5)\n",
    "u6 = concatenate([u6, c4])\n",
    "c6 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u6)\n",
    "c6 = Dropout(0.2) (c6)\n",
    "c6 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c6)\n",
    "\n",
    "u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c6)\n",
    "u7 = concatenate([u7, c3])\n",
    "c7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u7)\n",
    "c7 = Dropout(0.2) (c7)\n",
    "c7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c7)\n",
    "\n",
    "u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c7)\n",
    "u8 = concatenate([u8, c2])\n",
    "c8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u8)\n",
    "c8 = Dropout(0.1) (c8)\n",
    "c8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c8)\n",
    "\n",
    "u9 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c8)\n",
    "u9 = concatenate([u9, c1], axis=3)\n",
    "c9 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u9)\n",
    "c9 = Dropout(0.1) (c9)\n",
    "c9 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c9)\n",
    "\n",
    "outputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)\n",
    "\n",
    "model = Model(inputs=[inputs], outputs=[outputs])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[\"accuracy\",f1])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: visualkeras in c:\\users\\diego\\.conda\\envs\\ev_geo\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: aggdraw>=1.3.11 in c:\\users\\diego\\.conda\\envs\\ev_geo\\lib\\site-packages (from visualkeras) (1.3.12)\n",
      "Requirement already satisfied: numpy>=1.18.1 in c:\\users\\diego\\.conda\\envs\\ev_geo\\lib\\site-packages (from visualkeras) (1.18.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\diego\\.conda\\envs\\ev_geo\\lib\\site-packages (from visualkeras) (8.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install visualkeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=750x207 at 0x275BC9F7BE0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import visualkeras\n",
    "from tensorflow.python.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, ZeroPadding2D\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "visualkeras.layered_view(model,legend=True, scale_xy=0.5, scale_z=0.5, max_z=7, to_file='network.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.1304 - accuracy: 0.9450 - f1: 0.7914\n",
      "Epoch 00001: f1 improved from -inf to 0.79144, saving model to D:/cbers_data/DataSetModelo\\\n",
      "WARNING:tensorflow:From C:\\Users\\diego\\.conda\\envs\\ev_geo\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\diego\\.conda\\envs\\ev_geo\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: D:/cbers_data/DataSetModelo\\assets\n",
      "528/528 [==============================] - 1017s 2s/step - loss: 0.1304 - accuracy: 0.9450 - f1: 0.7914 - val_loss: 0.0643 - val_accuracy: 0.9653 - val_f1: 0.7842\n",
      "Epoch 2/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0920 - accuracy: 0.9561 - f1: 0.8372\n",
      "Epoch 00002: f1 improved from 0.79144 to 0.83720, saving model to D:/cbers_data/DataSetModelo\\\n",
      "INFO:tensorflow:Assets written to: D:/cbers_data/DataSetModelo\\assets\n",
      "528/528 [==============================] - 1006s 2s/step - loss: 0.0920 - accuracy: 0.9561 - f1: 0.8372 - val_loss: 0.1026 - val_accuracy: 0.9594 - val_f1: 0.7134\n",
      "Epoch 3/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.1093 - accuracy: 0.9544 - f1: 0.8426\n",
      "Epoch 00003: f1 improved from 0.83720 to 0.84257, saving model to D:/cbers_data/DataSetModelo\\\n",
      "INFO:tensorflow:Assets written to: D:/cbers_data/DataSetModelo\\assets\n",
      "528/528 [==============================] - 1045s 2s/step - loss: 0.1093 - accuracy: 0.9544 - f1: 0.8426 - val_loss: 0.0488 - val_accuracy: 0.9697 - val_f1: 0.7946\n",
      "Epoch 4/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0779 - accuracy: 0.9603 - f1: 0.8677\n",
      "Epoch 00004: f1 improved from 0.84257 to 0.86774, saving model to D:/cbers_data/DataSetModelo\\\n",
      "INFO:tensorflow:Assets written to: D:/cbers_data/DataSetModelo\\assets\n",
      "528/528 [==============================] - 1206s 2s/step - loss: 0.0779 - accuracy: 0.9603 - f1: 0.8677 - val_loss: 0.0520 - val_accuracy: 0.9699 - val_f1: 0.7891\n",
      "Epoch 5/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0766 - accuracy: 0.9602 - f1: 0.8547\n",
      "Epoch 00005: f1 did not improve from 0.86774\n",
      "528/528 [==============================] - 1185s 2s/step - loss: 0.0766 - accuracy: 0.9602 - f1: 0.8547 - val_loss: 0.0431 - val_accuracy: 0.9697 - val_f1: 0.7835\n",
      "Epoch 6/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0720 - accuracy: 0.9621 - f1: 0.8710\n",
      "Epoch 00006: f1 improved from 0.86774 to 0.87097, saving model to D:/cbers_data/DataSetModelo\\\n",
      "INFO:tensorflow:Assets written to: D:/cbers_data/DataSetModelo\\assets\n",
      "528/528 [==============================] - 1186s 2s/step - loss: 0.0720 - accuracy: 0.9621 - f1: 0.8710 - val_loss: 0.0389 - val_accuracy: 0.9752 - val_f1: 0.8585\n",
      "Epoch 7/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0765 - accuracy: 0.9607 - f1: 0.8547\n",
      "Epoch 00007: f1 did not improve from 0.87097\n",
      "528/528 [==============================] - 970s 2s/step - loss: 0.0765 - accuracy: 0.9607 - f1: 0.8547 - val_loss: 0.0536 - val_accuracy: 0.9741 - val_f1: 0.7975\n",
      "Epoch 8/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0690 - accuracy: 0.9628 - f1: 0.8726\n",
      "Epoch 00008: f1 improved from 0.87097 to 0.87260, saving model to D:/cbers_data/DataSetModelo\\\n",
      "INFO:tensorflow:Assets written to: D:/cbers_data/DataSetModelo\\assets\n",
      "528/528 [==============================] - 1059s 2s/step - loss: 0.0690 - accuracy: 0.9628 - f1: 0.8726 - val_loss: 0.0437 - val_accuracy: 0.9768 - val_f1: 0.8293\n",
      "Epoch 9/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0694 - accuracy: 0.9628 - f1: 0.8700\n",
      "Epoch 00009: f1 did not improve from 0.87260\n",
      "528/528 [==============================] - 1100s 2s/step - loss: 0.0694 - accuracy: 0.9628 - f1: 0.8700 - val_loss: 0.0414 - val_accuracy: 0.9736 - val_f1: 0.8044\n",
      "Epoch 10/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0675 - accuracy: 0.9642 - f1: 0.8717\n",
      "Epoch 00010: f1 did not improve from 0.87260\n",
      "528/528 [==============================] - 1081s 2s/step - loss: 0.0675 - accuracy: 0.9642 - f1: 0.8717 - val_loss: 0.0470 - val_accuracy: 0.9763 - val_f1: 0.8157\n",
      "Epoch 11/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9626 - f1: 0.8723\n",
      "Epoch 00011: f1 did not improve from 0.87260\n",
      "528/528 [==============================] - 894s 2s/step - loss: 0.0703 - accuracy: 0.9626 - f1: 0.8723 - val_loss: 0.0454 - val_accuracy: 0.9674 - val_f1: 0.8082\n",
      "Epoch 12/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0658 - accuracy: 0.9647 - f1: 0.8670\n",
      "Epoch 00012: f1 did not improve from 0.87260\n",
      "528/528 [==============================] - 891s 2s/step - loss: 0.0658 - accuracy: 0.9647 - f1: 0.8670 - val_loss: 0.0379 - val_accuracy: 0.9732 - val_f1: 0.7687\n",
      "Epoch 13/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0663 - accuracy: 0.9633 - f1: 0.8704\n",
      "Epoch 00013: f1 did not improve from 0.87260\n",
      "528/528 [==============================] - 909s 2s/step - loss: 0.0663 - accuracy: 0.9633 - f1: 0.8704 - val_loss: 0.0426 - val_accuracy: 0.9717 - val_f1: 0.8509\n",
      "Epoch 14/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0653 - accuracy: 0.9656 - f1: 0.8886\n",
      "Epoch 00014: f1 improved from 0.87260 to 0.88862, saving model to D:/cbers_data/DataSetModelo\\\n",
      "INFO:tensorflow:Assets written to: D:/cbers_data/DataSetModelo\\assets\n",
      "528/528 [==============================] - 897s 2s/step - loss: 0.0653 - accuracy: 0.9656 - f1: 0.8886 - val_loss: 0.0434 - val_accuracy: 0.9724 - val_f1: 0.8832\n",
      "Epoch 15/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9630 - f1: 0.8735\n",
      "Epoch 00015: f1 did not improve from 0.88862\n",
      "528/528 [==============================] - 893s 2s/step - loss: 0.0686 - accuracy: 0.9630 - f1: 0.8735 - val_loss: 0.0414 - val_accuracy: 0.9745 - val_f1: 0.8076\n",
      "Epoch 16/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0645 - accuracy: 0.9650 - f1: 0.8789\n",
      "Epoch 00016: f1 did not improve from 0.88862\n",
      "528/528 [==============================] - 895s 2s/step - loss: 0.0645 - accuracy: 0.9650 - f1: 0.8789 - val_loss: 0.0675 - val_accuracy: 0.9604 - val_f1: 0.7501\n",
      "Epoch 17/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0648 - accuracy: 0.9660 - f1: 0.8886\n",
      "Epoch 00017: f1 improved from 0.88862 to 0.88864, saving model to D:/cbers_data/DataSetModelo\\\n",
      "INFO:tensorflow:Assets written to: D:/cbers_data/DataSetModelo\\assets\n",
      "528/528 [==============================] - 902s 2s/step - loss: 0.0648 - accuracy: 0.9660 - f1: 0.8886 - val_loss: 0.0400 - val_accuracy: 0.9768 - val_f1: 0.8349\n",
      "Epoch 18/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9623 - f1: 0.8662\n",
      "Epoch 00018: f1 did not improve from 0.88864\n",
      "528/528 [==============================] - 897s 2s/step - loss: 0.0686 - accuracy: 0.9623 - f1: 0.8662 - val_loss: 0.0426 - val_accuracy: 0.9796 - val_f1: 0.8370\n",
      "Epoch 19/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0646 - accuracy: 0.9655 - f1: 0.8734\n",
      "Epoch 00019: f1 did not improve from 0.88864\n",
      "528/528 [==============================] - 1002s 2s/step - loss: 0.0646 - accuracy: 0.9655 - f1: 0.8734 - val_loss: 0.0358 - val_accuracy: 0.9769 - val_f1: 0.8340\n",
      "Epoch 20/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0604 - accuracy: 0.9662 - f1: 0.8782\n",
      "Epoch 00020: f1 did not improve from 0.88864\n",
      "528/528 [==============================] - 1101s 2s/step - loss: 0.0604 - accuracy: 0.9662 - f1: 0.8782 - val_loss: 0.0528 - val_accuracy: 0.9661 - val_f1: 0.7822\n",
      "Epoch 21/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0826 - accuracy: 0.9625 - f1: 0.8655\n",
      "Epoch 00021: f1 did not improve from 0.88864\n",
      "528/528 [==============================] - 953s 2s/step - loss: 0.0826 - accuracy: 0.9625 - f1: 0.8655 - val_loss: 0.0413 - val_accuracy: 0.9804 - val_f1: 0.8452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0629 - accuracy: 0.9650 - f1: 0.8766\n",
      "Epoch 00022: f1 did not improve from 0.88864\n",
      "528/528 [==============================] - 902s 2s/step - loss: 0.0629 - accuracy: 0.9650 - f1: 0.8766 - val_loss: 0.0828 - val_accuracy: 0.9634 - val_f1: 0.7520\n",
      "Epoch 23/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0660 - accuracy: 0.9640 - f1: 0.8712\n",
      "Epoch 00023: f1 did not improve from 0.88864\n",
      "528/528 [==============================] - 912s 2s/step - loss: 0.0660 - accuracy: 0.9640 - f1: 0.8712 - val_loss: 0.0450 - val_accuracy: 0.9737 - val_f1: 0.8380\n",
      "Epoch 24/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0609 - accuracy: 0.9663 - f1: 0.8890\n",
      "Epoch 00024: f1 improved from 0.88864 to 0.88901, saving model to D:/cbers_data/DataSetModelo\\\n",
      "INFO:tensorflow:Assets written to: D:/cbers_data/DataSetModelo\\assets\n",
      "528/528 [==============================] - 976s 2s/step - loss: 0.0609 - accuracy: 0.9663 - f1: 0.8890 - val_loss: 0.0350 - val_accuracy: 0.9764 - val_f1: 0.8298\n",
      "Epoch 25/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0584 - accuracy: 0.9678 - f1: 0.8867\n",
      "Epoch 00025: f1 did not improve from 0.88901\n",
      "528/528 [==============================] - 964s 2s/step - loss: 0.0584 - accuracy: 0.9678 - f1: 0.8867 - val_loss: 0.0369 - val_accuracy: 0.9792 - val_f1: 0.8202\n",
      "Epoch 26/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0632 - accuracy: 0.9640 - f1: 0.8777\n",
      "Epoch 00026: f1 did not improve from 0.88901\n",
      "528/528 [==============================] - 961s 2s/step - loss: 0.0632 - accuracy: 0.9640 - f1: 0.8777 - val_loss: 0.0532 - val_accuracy: 0.9675 - val_f1: 0.7794\n",
      "Epoch 27/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0594 - accuracy: 0.9659 - f1: 0.8821\n",
      "Epoch 00027: f1 did not improve from 0.88901\n",
      "528/528 [==============================] - 965s 2s/step - loss: 0.0594 - accuracy: 0.9659 - f1: 0.8821 - val_loss: 0.0300 - val_accuracy: 0.9804 - val_f1: 0.8712\n",
      "Epoch 28/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0652 - accuracy: 0.9661 - f1: 0.8849\n",
      "Epoch 00028: f1 did not improve from 0.88901\n",
      "528/528 [==============================] - 961s 2s/step - loss: 0.0652 - accuracy: 0.9661 - f1: 0.8849 - val_loss: 0.0532 - val_accuracy: 0.9683 - val_f1: 0.8156\n",
      "Epoch 29/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0576 - accuracy: 0.9659 - f1: 0.9001\n",
      "Epoch 00029: f1 improved from 0.88901 to 0.90011, saving model to D:/cbers_data/DataSetModelo\\\n",
      "INFO:tensorflow:Assets written to: D:/cbers_data/DataSetModelo\\assets\n",
      "528/528 [==============================] - 967s 2s/step - loss: 0.0576 - accuracy: 0.9659 - f1: 0.9001 - val_loss: 0.0402 - val_accuracy: 0.9777 - val_f1: 0.8360\n",
      "Epoch 30/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0611 - accuracy: 0.9645 - f1: 0.8887\n",
      "Epoch 00030: f1 did not improve from 0.90011\n",
      "528/528 [==============================] - 962s 2s/step - loss: 0.0611 - accuracy: 0.9645 - f1: 0.8887 - val_loss: 0.0380 - val_accuracy: 0.9786 - val_f1: 0.8514\n",
      "Epoch 31/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0604 - accuracy: 0.9663 - f1: 0.8894\n",
      "Epoch 00031: f1 did not improve from 0.90011\n",
      "528/528 [==============================] - 964s 2s/step - loss: 0.0604 - accuracy: 0.9663 - f1: 0.8894 - val_loss: 0.0446 - val_accuracy: 0.9770 - val_f1: 0.8466\n",
      "Epoch 32/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0573 - accuracy: 0.9685 - f1: 0.8865\n",
      "Epoch 00032: f1 did not improve from 0.90011\n",
      "528/528 [==============================] - 965s 2s/step - loss: 0.0573 - accuracy: 0.9685 - f1: 0.8865 - val_loss: 0.0411 - val_accuracy: 0.9724 - val_f1: 0.8272\n",
      "Epoch 33/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0624 - accuracy: 0.9635 - f1: 0.8793\n",
      "Epoch 00033: f1 did not improve from 0.90011\n",
      "528/528 [==============================] - 965s 2s/step - loss: 0.0624 - accuracy: 0.9635 - f1: 0.8793 - val_loss: 0.0487 - val_accuracy: 0.9746 - val_f1: 0.8215\n",
      "Epoch 34/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0572 - accuracy: 0.9686 - f1: 0.8836\n",
      "Epoch 00034: f1 did not improve from 0.90011\n",
      "528/528 [==============================] - 969s 2s/step - loss: 0.0572 - accuracy: 0.9686 - f1: 0.8836 - val_loss: 0.0452 - val_accuracy: 0.9748 - val_f1: 0.7925\n",
      "Epoch 35/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0603 - accuracy: 0.9646 - f1: 0.8835\n",
      "Epoch 00035: f1 did not improve from 0.90011\n",
      "528/528 [==============================] - 964s 2s/step - loss: 0.0603 - accuracy: 0.9646 - f1: 0.8835 - val_loss: 0.0483 - val_accuracy: 0.9736 - val_f1: 0.8248\n",
      "Epoch 36/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0558 - accuracy: 0.9677 - f1: 0.8941\n",
      "Epoch 00036: f1 did not improve from 0.90011\n",
      "528/528 [==============================] - 965s 2s/step - loss: 0.0558 - accuracy: 0.9677 - f1: 0.8941 - val_loss: 0.0442 - val_accuracy: 0.9721 - val_f1: 0.7743\n",
      "Epoch 37/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0586 - accuracy: 0.9672 - f1: 0.8914\n",
      "Epoch 00037: f1 did not improve from 0.90011\n",
      "528/528 [==============================] - 961s 2s/step - loss: 0.0586 - accuracy: 0.9672 - f1: 0.8914 - val_loss: 0.0346 - val_accuracy: 0.9751 - val_f1: 0.8655\n",
      "Epoch 38/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0597 - accuracy: 0.9657 - f1: 0.8868\n",
      "Epoch 00038: f1 did not improve from 0.90011\n",
      "528/528 [==============================] - 963s 2s/step - loss: 0.0597 - accuracy: 0.9657 - f1: 0.8868 - val_loss: 0.0363 - val_accuracy: 0.9769 - val_f1: 0.8354\n",
      "Epoch 39/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0580 - accuracy: 0.9675 - f1: 0.8903\n",
      "Epoch 00039: f1 did not improve from 0.90011\n",
      "528/528 [==============================] - 973s 2s/step - loss: 0.0580 - accuracy: 0.9675 - f1: 0.8903 - val_loss: 0.0548 - val_accuracy: 0.9689 - val_f1: 0.7581\n",
      "Epoch 40/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0558 - accuracy: 0.9666 - f1: 0.8909\n",
      "Epoch 00040: f1 did not improve from 0.90011\n",
      "528/528 [==============================] - 963s 2s/step - loss: 0.0558 - accuracy: 0.9666 - f1: 0.8909 - val_loss: 0.0309 - val_accuracy: 0.9799 - val_f1: 0.8370\n",
      "Epoch 41/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0590 - accuracy: 0.9676 - f1: 0.8914\n",
      "Epoch 00041: f1 did not improve from 0.90011\n",
      "528/528 [==============================] - 965s 2s/step - loss: 0.0590 - accuracy: 0.9676 - f1: 0.8914 - val_loss: 0.0510 - val_accuracy: 0.9740 - val_f1: 0.7964\n",
      "Epoch 42/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0559 - accuracy: 0.9662 - f1: 0.8860\n",
      "Epoch 00042: f1 did not improve from 0.90011\n",
      "528/528 [==============================] - 966s 2s/step - loss: 0.0559 - accuracy: 0.9662 - f1: 0.8860 - val_loss: 0.0378 - val_accuracy: 0.9766 - val_f1: 0.8387\n",
      "Epoch 43/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0522 - accuracy: 0.9687 - f1: 0.9006\n",
      "Epoch 00043: f1 improved from 0.90011 to 0.90059, saving model to D:/cbers_data/DataSetModelo\\\n",
      "INFO:tensorflow:Assets written to: D:/cbers_data/DataSetModelo\\assets\n",
      "528/528 [==============================] - 974s 2s/step - loss: 0.0522 - accuracy: 0.9687 - f1: 0.9006 - val_loss: 0.0372 - val_accuracy: 0.9727 - val_f1: 0.8590\n",
      "Epoch 44/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0561 - accuracy: 0.9670 - f1: 0.8914\n",
      "Epoch 00044: f1 did not improve from 0.90059\n",
      "528/528 [==============================] - 965s 2s/step - loss: 0.0561 - accuracy: 0.9670 - f1: 0.8914 - val_loss: 0.0391 - val_accuracy: 0.9767 - val_f1: 0.7944\n",
      "Epoch 45/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0598 - accuracy: 0.9669 - f1: 0.8988\n",
      "Epoch 00045: f1 did not improve from 0.90059\n",
      "528/528 [==============================] - 949s 2s/step - loss: 0.0598 - accuracy: 0.9669 - f1: 0.8988 - val_loss: 0.0616 - val_accuracy: 0.9671 - val_f1: 0.7663\n",
      "Epoch 46/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0538 - accuracy: 0.9672 - f1: 0.8990\n",
      "Epoch 00046: f1 did not improve from 0.90059\n",
      "528/528 [==============================] - 949s 2s/step - loss: 0.0538 - accuracy: 0.9672 - f1: 0.8990 - val_loss: 0.0394 - val_accuracy: 0.9725 - val_f1: 0.8206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0564 - accuracy: 0.9654 - f1: 0.8968\n",
      "Epoch 00047: f1 did not improve from 0.90059\n",
      "528/528 [==============================] - 948s 2s/step - loss: 0.0564 - accuracy: 0.9654 - f1: 0.8968 - val_loss: 0.0487 - val_accuracy: 0.9708 - val_f1: 0.8191\n",
      "Epoch 48/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0577 - accuracy: 0.9664 - f1: 0.8843\n",
      "Epoch 00048: f1 did not improve from 0.90059\n",
      "528/528 [==============================] - 953s 2s/step - loss: 0.0577 - accuracy: 0.9664 - f1: 0.8843 - val_loss: 0.0457 - val_accuracy: 0.9754 - val_f1: 0.8334\n",
      "Epoch 49/64\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.0542 - accuracy: 0.9665 - f1: 0.8926\n",
      "Epoch 00049: f1 did not improve from 0.90059\n",
      "528/528 [==============================] - 952s 2s/step - loss: 0.0542 - accuracy: 0.9665 - f1: 0.8926 - val_loss: 0.0352 - val_accuracy: 0.9748 - val_f1: 0.8079\n",
      "Epoch 00049: early stopping\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "model_history = model.fit(train_gen, \n",
    "                    epochs = 64, \n",
    "                    batch_size = BATCH_SIZE, \n",
    "                    validation_data = val_gen, \n",
    "                    callbacks = callbacks_list,\n",
    "                    steps_per_epoch =(NO_OF_TRAINING_IMAGES/BATCH_SIZE), \n",
    "                    validation_steps = (NO_OF_VAL_IMAGES/BATCH_SIZE)\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import PIL\n",
    "from PIL import ImageOps\n",
    "\n",
    "\n",
    "def get_test_batch(qnt_images):\n",
    "    evaluate = []\n",
    "    \n",
    "    convert_to_image = keras.preprocessing.image.array_to_img\n",
    "    dir_images = test_frame_path\n",
    "    dir_masks = test_mask_path\n",
    "    test_files = os.listdir(test_frame_path)\n",
    "    #batch_images_structure\n",
    "    mask_true_image_test = np.zeros((qnt_images,256,256,1))\n",
    "    batch_test_images = np.zeros((qnt_images,256,256,3))\n",
    "\n",
    "    for i in range(qnt_images):\n",
    "        #mask_test = np.load(dir_masks + '/'+ test_files[i], allow_pickle=True)\n",
    "        #mask_test = mask_test / 1\n",
    "        #mask_test = cv2.resize(mask_test, (512,512))\n",
    "        #mask_test = np.expand_dims(mask_test,axis=2)\n",
    "        #mask_test = normalize_min_max(mask_test)\n",
    "        mask_test = gdal.Open(dir_masks + '/'+ test_files[i]).ReadAsArray()\n",
    "        #ask_test = mask_test.transpose((1,2,0))\n",
    "        mask_test = cv2.normalize(mask_test, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "\n",
    "        \n",
    "        #imagem_teste = np.load(dir_images + '/'+ test_files[i],allow_pickle=True)\n",
    "        imagem_teste = gdal.Open(dir_images + '/'+ test_files[i]).ReadAsArray()\n",
    "        imagem_teste = imagem_teste.transpose((1,2,0))\n",
    "        imagem_teste = cv2.normalize(imagem_teste, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "\n",
    "        #imagem_teste = cv2.resize(imagem_teste, (512, 512))\n",
    "        #imagem_teste = cv2.resize(imagem_teste, (512, 512))\n",
    "        #imagem_teste = normalize_min_max(imagem_teste)\n",
    "        \n",
    "        y_eval = np.expand_dims(mask_test, axis=0)\n",
    "        x_eval = np.expand_dims(imagem_teste, axis=0)\n",
    "        \n",
    "        evaluate.append(model.evaluate(x=x_eval,y=y_eval))\n",
    "        \n",
    "        mask_true_image_test [i,:,:] = mask_test\n",
    "        batch_test_images [i,:,:] = imagem_teste\n",
    "    return batch_test_images, mask_true_image_test, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 997us/step - loss: 0.0116 - accuracy: 0.9966 - f1: 0.8547\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (256,256) into shape (512,512,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-a7222ee366b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mqnt_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbatch_test_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask_true_image_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_test_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqnt_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-25-604b28f61d3b>\u001b[0m in \u001b[0;36mget_test_batch\u001b[1;34m(qnt_images)\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mevaluate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_eval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_eval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mmask_true_image_test\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmask_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[0mbatch_test_images\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimagem_teste\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mbatch_test_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask_true_image_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (256,256) into shape (512,512,1)"
     ]
    }
   ],
   "source": [
    "qnt_images = 15\n",
    "batch_test_images, mask_true_image_test, evaluate = get_test_batch(qnt_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_eval = pd.DataFrame(evaluate)\n",
    "df_eval.columns = [\"Loss\", \"Accuracy\", \"f1\"]\n",
    "df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( 'Loss: ' + str(df_eval['Loss'].mean()))\n",
    "print( 'F1: ' + str(df_eval['f1'].mean()))\n",
    "print( 'Accuracy: ' + str(df_eval['Accuracy'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_predict = model.predict(batch_test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(display_list):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "\n",
    "    \n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i+1)\n",
    "        plt.title(title[i])\n",
    "        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
    "        plt.axis('off')\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(qnt_images):\n",
    "    b, g, r    = batch_test_images[i][:, :, 1], batch_test_images[i][:, :, 2], batch_test_images[i][:, :, 3]\n",
    "    rgb = np.dstack((r,g,b))\n",
    "    show_images([rgb, mask_true_image_test[i], mask_predict[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc = model_history.history['accuracy']\n",
    "val_acc = model_history.history['val_accuracy']\n",
    "\n",
    "loss = model_history.history['loss']\n",
    "val_loss = model_history.history['val_loss']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model_history.history['loss']\n",
    "val_loss = model_history.history['val_loss']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = model_history.history['f1']\n",
    "val_f1 = model_history.history['val_f1']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(f1, label='Training F1-Score')\n",
    "plt.plot(val_f1, label='Validation F1-Score')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('F1-Score')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation F1-Score')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
